{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Soft deadline: `30.03.2022 23:59`"
      ],
      "metadata": {
        "id": "nXpkXz1QJmhJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxy7euTpCbGp"
      },
      "source": [
        "In this homework you will understand the fine-tuning procedure and get acquainted with Huggingface Datasets library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1MXcMymXeCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6c81b5-9e80-497a-b762-07c46dc8426a"
      },
      "source": [
        "! pip install datasets\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 325 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 60.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FykUK-TFXf-2"
      },
      "source": [
        "For our goals we will use [Datasets](https://huggingface.co/docs/datasets/) library and take `yahoo_answers_topics` dataset - the task of this dataset is to divide documents on 10 topic categories. More detiled information can be found on the dataset [page](https://huggingface.co/datasets/viewer/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebsFAQsgXNB0"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import load_metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('yahoo_answers_topics')"
      ],
      "metadata": {
        "id": "k6LXFAjmZDn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.remove_columns(['question_title','question_content','id'])\n",
        "metric = load_metric('f1')"
      ],
      "metadata": {
        "id": "aT5N6OuNjj5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train=dataset['train']#.shard(224,index=1)\n",
        "ds_test=dataset['test']#.shard(20,index=1)"
      ],
      "metadata": {
        "id": "cd5joK6HC4nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train"
      ],
      "metadata": {
        "id": "NFI-G6Stdk59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test"
      ],
      "metadata": {
        "id": "D0ESlRMcnDMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U4YUOB5W8uG"
      },
      "source": [
        "# Fine-tuning the model** (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYIq9l7CYBR"
      },
      "source": [
        "from transformers import (ElectraTokenizer, ElectraForSequenceClassification, ElectraTokenizerFast,\n",
        "                          get_scheduler, pipeline, ElectraForMaskedLM, ElectraModel, AdamW, TrainingArguments, Trainer)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_metric\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElZ6k36rb0VG"
      },
      "source": [
        "Fine-tuning procedure on the end task consists of adding additional layers on the top of the pre-trained model. The resulting model can be tuned fully (passing gradients through the all model) or partially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNEmksaPb3Uu"
      },
      "source": [
        "**Task**: \n",
        "- load tokenizer and model\n",
        "- look at the predictions of the model as-is before any fine-tuning\n",
        "\n",
        "\n",
        "```\n",
        "- Why don't you ask [MASK]?\n",
        "- What is [MASK]\n",
        "- Let's talk about [MASK] physics\n",
        "```\n",
        "\n",
        "- convert `best_answer` to the input tokens (supporting function for dataset is provided below) \n",
        "\n",
        "```\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"best_answer\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "```\n",
        "\n",
        "- define optimizer, sheduler (optional)\n",
        "- fine-tune the model (write the training loop), plot the loss changes and measure results in terms of weighted F1 score\n",
        "- get the masked word prediction (sample sentences above) on the fine-tuned model, why the results as they are and what should be done in order to change that (write down your answer)\n",
        "- Tune the training hyperparameters (and write down your results).\n",
        "\n",
        "**Tips**:\n",
        "- The easiest way to get predictions is to use transformers `pipeline` function \n",
        "- Do not forget to set `num_labels` parameter, when initializing the model\n",
        "- To convert data to batches use `DataLoader`\n",
        "- Even the `small` version of Electra can be long to train, so you can take data sample (>= 5000 and set seed for reproducibility)\n",
        "- You may want to try freezing (do not update the pretrained model weights) all the layers exept the ones for classification, in that case use:\n",
        "\n",
        "\n",
        "```\n",
        "for param in model.electra.parameters():\n",
        "      param.requires_grad = False\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained model performance"
      ],
      "metadata": {
        "id": "YfmFMSXADk8d"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqAAFqZcwbu"
      },
      "source": [
        "MODEL_NAME = \"google/electra-small-generator\"\n",
        "TOKENIZER_NAME = \"google/electra-small-generator\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_initial=ElectraForMaskedLM.from_pretrained(MODEL_NAME)\n",
        "tokenizer=ElectraTokenizerFast.from_pretrained(TOKENIZER_NAME)"
      ],
      "metadata": {
        "id": "x0pAASOb3_UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasking = pipeline(\n",
        "    task = \"fill-mask\",\n",
        "    model = MODEL_NAME,\n",
        "    tokenizer=TOKENIZER_NAME,\n",
        "    top_k=3,\n",
        ")"
      ],
      "metadata": {
        "id": "6jo-R5fBDvcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech1 = f\"- Why don't you ask {unmasking.tokenizer.mask_token}?\"\n",
        "speech2 = f\"- What is {unmasking.tokenizer.mask_token}?\"\n",
        "speech3 = f\"- Let's talk about {unmasking.tokenizer.mask_token} physics\""
      ],
      "metadata": {
        "id": "F4quQqJdG0jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(unmasking(speech1))"
      ],
      "metadata": {
        "id": "nI6uKnA1G6DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier training"
      ],
      "metadata": {
        "id": "A79GwvfrEFPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing answers in train-test datasets\n",
        "def tokenize_function(examples):\n",
        "    model_inputs=tokenizer(examples[\"best_answer\"], padding=\"max_length\", truncation=True)\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets_train = ds_train.map(tokenize_function,batched=True)\n",
        "tokenized_datasets_train=tokenized_datasets_train.rename_column('topic','label')\n",
        "tokenized_datasets_test = ds_test.map(tokenize_function,batched=True)\n",
        "tokenized_datasets_test=tokenized_datasets_test.rename_column('topic','label')"
      ],
      "metadata": {
        "id": "rnidJhSLDJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining untrained electra classifier with 10 possible labels (topics)\n",
        "electra_classifier = ElectraForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=10)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(electra_classifier.parameters(), lr=1e-3, weight_decay=0.000)\n",
        "num_epoch=5"
      ],
      "metadata": {
        "id": "LMTZEw-_7KPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining untrained electra classifier with 10 possible labels (topics)\n",
        "electra_classifier = ElectraForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=10)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "# electra_classifier.classifier.dense=torch.nn.Linear(256, 64)\n",
        "# electra_classifier.classifier.out_proj=torch.nn.Sequential(\n",
        "#     torch.nn.LeakyReLU(),\n",
        "#     torch.nn.Linear(64, 10))\n",
        "for param in electra_classifier.electra.parameters():\n",
        "      param.requires_grad = False\n",
        "optimizer = torch.optim.AdamW(electra_classifier.classifier.parameters(), lr=1e-3, weight_decay=0.0005)\n",
        "num_epoch=10"
      ],
      "metadata": {
        "id": "aXLIl7ulorYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "electra_classifier.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBRIsAPL5uat",
        "outputId": "b520afb0-5270-4ecd-bce2-8847feaaeddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraClassificationHead(\n",
              "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (out_proj): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "electra_classifier.to('cuda:0')"
      ],
      "metadata": {
        "id": "3x7hiaPJJZ5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devic='cuda:0'\n",
        "for epoch in range(num_epoch):\n",
        "    losses = []\n",
        "    electra_classifier.train()\n",
        "    for i, data in enumerate(tokenized_datasets_train):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = torch.tensor([data['input_ids']],device=devic)\n",
        "        attention_mask = torch.tensor([data['attention_mask']], device=devic)\n",
        "        label = torch.tensor([data['label']], device=devic)\n",
        "\n",
        "        out=electra_classifier(input_ids,attention_mask)\n",
        "        loss_value=loss(out[0],label)\n",
        "        loss_value.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss_value)\n",
        "        #print(loss_value)\n",
        "    print(f\"Epoch {epoch}\\n Current loss {torch.mean(torch.tensor(losses))}\\n\")\n",
        "    preds=testify(tokenized_datasets_test)\n",
        "    print(\"F1:\",f1_score(tokenized_datasets_test['label'], preds, average='weighted'))"
      ],
      "metadata": {
        "id": "kLD4UQ20wEyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testify(dataset):\n",
        "  pred_labels=[]\n",
        "  for i,data in enumerate(dataset):\n",
        "    with torch.no_grad():\n",
        "      input_ids = torch.tensor([data['input_ids']],device=devic)\n",
        "      attention_mask = torch.tensor([data['attention_mask']], device=devic)\n",
        "      out = electra_classifier(input_ids,attention_mask)\n",
        "      pred_labels.append(out[0].argmax().cpu().numpy())\n",
        "\n",
        "  return pred_labels"
      ],
      "metadata": {
        "id": "y93Bniv1iHHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=testify(tokenized_datasets_test)"
      ],
      "metadata": {
        "id": "Fhh_WZ-EfQXH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "a2749fd9-7a4e-4b56-8ad5-37ebaa4d39ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-3c5792e3d904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_datasets_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-111-1f7ce4171579>\u001b[0m in \u001b[0;36mtestify\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melectra_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mpred_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpred_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(tokenized_datasets_test['label'], preds, average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4MAYZtqjAYj",
        "outputId": "eb9b6354-c18f-41a6-ae7c-8622558061f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3360615863978749"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "electra_classifier.save_pretrained('./drive/MyDrive/NLP/mymodel')"
      ],
      "metadata": {
        "id": "4GVMv8iNeP6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ElectraForMaskedLM.from_pretrained('./drive/MyDrive/NLP/mymodel')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2Sc0OIDerXb",
        "outputId": "fb973fca-2c6c-4dc6-de5f-ffffc300a494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./drive/MyDrive/NLP/mymodel were not used when initializing ElectraForMaskedLM: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "- This IS expected if you are initializing ElectraForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at ./drive/MyDrive/NLP/mymodel and are newly initialized: ['generator_predictions.dense.bias', 'generator_lm_head.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unmasking1 = pipeline(\n",
        "    task = \"fill-mask\",\n",
        "    model = './drive/MyDrive/NLP/mymodel',\n",
        "    tokenizer=TOKENIZER_NAME,\n",
        "    top_k=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyn1j7n6ezyT",
        "outputId": "e31ae3ce-a1a8-4be9-dad7-19330ba0ee73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./drive/MyDrive/NLP/mymodel were not used when initializing ElectraForMaskedLM: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "- This IS expected if you are initializing ElectraForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at ./drive/MyDrive/NLP/mymodel and are newly initialized: ['generator_predictions.dense.bias', 'generator_lm_head.bias', 'generator_lm_head.weight', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speech1 = f\"- Why don't you ask {unmasking1.tokenizer.mask_token}?\"\n",
        "speech2 = f\"- What is {unmasking1.tokenizer.mask_token}?\"\n",
        "speech3 = f\"- Let's talk about {unmasking1.tokenizer.mask_token} physics\""
      ],
      "metadata": {
        "id": "MJW6ri5ZfG27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(unmasking1(speech1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stDmip6me-df",
        "outputId": "8e3e6761-cda7-4737-e127-0a0b55a75ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.0004469767736736685,\n",
            "  'sequence': \"- why don't you ask peeling?\",\n",
            "  'token': 28241,\n",
            "  'token_str': 'peeling'},\n",
            " {'score': 0.00037571616121567786,\n",
            "  'sequence': \"- why don't you askutter?\",\n",
            "  'token': 26878,\n",
            "  'token_str': '##utter'},\n",
            " {'score': 0.0003629341081250459,\n",
            "  'sequence': \"- why don't you ask roaming?\",\n",
            "  'token': 24430,\n",
            "  'token_str': 'roaming'}]\n"
          ]
        }
      ]
    }
  ]
}